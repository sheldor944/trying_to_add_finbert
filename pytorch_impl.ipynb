{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting model training and evaluation...\n",
      "\n",
      "Iteration 1/1\n",
      "Epoch [10/50], Loss: 0.0006\n",
      "Epoch [20/50], Loss: 0.0005\n",
      "Epoch [30/50], Loss: 0.0005\n",
      "Epoch [40/50], Loss: 0.0005\n",
      "Epoch [50/50], Loss: 0.0006\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 42.85 GiB. GPU 0 has a total capacity of 3.80 GiB of which 574.31 MiB is free. Process 10728 has 154.00 MiB memory in use. Process 3866445 has 664.00 MiB memory in use. Including non-PyTorch memory, this process has 2.43 GiB memory in use. Of the allocated memory 2.30 GiB is allocated by PyTorch, and 27.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 243\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirection Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 230\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    223\u001b[39m X_train, y_train, X_test, y_test, scaler = load_and_preprocess_data(\n\u001b[32m    224\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mXAU_15m_data.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m     split=\u001b[32m0.85\u001b[39m,\n\u001b[32m    226\u001b[39m     sequence_length=SEQUENCE_LENGTH\n\u001b[32m    227\u001b[39m )\n\u001b[32m    229\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting model training and evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m mae, mape, acc, dir_acc, predictions = \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_ITERATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQUENCE_LENGTH\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Average Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    237\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean Absolute Error = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 190\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, scaler, n_iterations, sequence_length)\u001b[39m\n\u001b[32m    187\u001b[39m train_model(model, train_loader, criterion, optimizer, epochs=\u001b[32m50\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m mae, mape, dir_acc, predictions = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Update totals\u001b[39;00m\n\u001b[32m    193\u001b[39m total_mae += mae\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, X_test, y_test, scaler)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    128\u001b[39m     X_test_tensor = torch.FloatTensor(X_test).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m)\u001b[49m.cpu().numpy()\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Inverse transform predictions and actual values\u001b[39;00m\n\u001b[32m    132\u001b[39m predictions_original = scaler.inverse_transform(predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis_torch_imp/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis_torch_imp/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mLSTMModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# First LSTM layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     x, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m     55\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis_torch_imp/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis_torch_imp/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/thesis_torch_imp/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 42.85 GiB. GPU 0 has a total capacity of 3.80 GiB of which 574.31 MiB is free. Process 10728 has 154.00 MiB memory in use. Process 3866445 has 664.00 MiB memory in use. Including non-PyTorch memory, this process has 2.43 GiB memory in use. Of the allocated memory 2.30 GiB is allocated by PyTorch, and 27.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Required versions:\n",
    "# torch>=2.0.0\n",
    "# numpy>=1.21.0\n",
    "# pandas>=1.3.0\n",
    "# scikit-learn>=0.24.0\n",
    "# matplotlib>=3.3.0\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X).to(device)\n",
    "        self.y = torch.FloatTensor(y).reshape(-1, 1).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, sequence_length, hidden_size=128):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=hidden_size, \n",
    "                           num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=64, \n",
    "                           num_layers=1, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=64, hidden_size=32, \n",
    "                           num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(sequence_length)\n",
    "        self.bn2 = nn.BatchNorm1d(sequence_length)\n",
    "        self.bn3 = nn.BatchNorm1d(sequence_length)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First LSTM layer\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Third LSTM layer\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Take the last time step\n",
    "        x = x[:, -1, :]\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def load_and_preprocess_data(file_path, split=0.8, sequence_length=10):\n",
    "    # Load the dataset\n",
    "    stock_data = pd.read_csv(file_path, delimiter=\";\")\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y.%m.%d %H:%M')\n",
    "    stock_data = stock_data.sort_values(by='Date')\n",
    "    \n",
    "    # Extract close prices\n",
    "    close_prices = stock_data['Close'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(close_prices) * split)\n",
    "    train = close_prices[:train_size]\n",
    "    test = close_prices[train_size:]\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "    X_test, y_test = create_sequences(test_scaled, sequence_length)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length)])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def calculate_direction_accuracy(actual_values, predicted_values):\n",
    "    actual_direction = (actual_values[1:] > actual_values[:-1]).astype(int)\n",
    "    predicted_direction = (predicted_values[1:] > predicted_values[:-1]).astype(int)\n",
    "    direction_accuracy = np.mean(actual_direction == predicted_direction)\n",
    "    return direction_accuracy, actual_direction, predicted_direction\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "def evaluate(model, X_test, y_test, scaler):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        predictions = model(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "    # Inverse transform predictions and actual values\n",
    "    predictions_original = scaler.inverse_transform(predictions)\n",
    "    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test_original, predictions_original)\n",
    "    mape = mean_absolute_percentage_error(y_test_original, predictions_original)\n",
    "    \n",
    "    # Calculate direction accuracy\n",
    "    dir_acc, actual_dir, pred_dir = calculate_direction_accuracy(y_test_original, predictions_original)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: Price predictions\n",
    "    ax1.plot(y_test_original, label=\"Actual Values\", color=\"blue\", marker='o')\n",
    "    ax1.plot(predictions_original, label=\"Predicted Values\", color=\"red\",\n",
    "            linestyle='dashed', marker='x')\n",
    "    ax1.set_title(\"Comparison of Predicted vs Actual Values\")\n",
    "    ax1.set_xlabel(\"Sample Index\")\n",
    "    ax1.set_ylabel(\"Price\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot 2: Direction predictions\n",
    "    ax2.plot(actual_dir, label=\"Actual Direction\", color=\"blue\", marker='o')\n",
    "    ax2.plot(pred_dir, label=\"Predicted Direction\", color=\"red\",\n",
    "            linestyle='dashed', marker='x')\n",
    "    ax2.set_title(\"Comparison of Predicted vs Actual Price Direction (1=Up, 0=Down)\")\n",
    "    ax2.set_xlabel(\"Sample Index\")\n",
    "    ax2.set_ylabel(\"Direction\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mae, mape, dir_acc, predictions_original\n",
    "\n",
    "def run_model(X_train, y_train, X_test, y_test, scaler, n_iterations=1, sequence_length=60):\n",
    "    total_mae = total_mape = total_acc = total_dir_acc = 0\n",
    "    best_predictions = None\n",
    "    best_metrics = float('inf')  # Using MAE as the metric to track\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        # Initialize model, criterion, and optimizer\n",
    "        model = LSTMModel(sequence_length).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nIteration {iteration + 1}/{n_iterations}\")\n",
    "        train_model(model, train_loader, criterion, optimizer, epochs=50)\n",
    "        \n",
    "        # Evaluate model\n",
    "        mae, mape, dir_acc, predictions = evaluate(model, X_test, y_test, scaler)\n",
    "        \n",
    "        # Update totals\n",
    "        total_mae += mae\n",
    "        total_mape += mape\n",
    "        total_acc += (1 - mape)\n",
    "        total_dir_acc += dir_acc\n",
    "        \n",
    "        # Track best model\n",
    "        if mae < best_metrics:\n",
    "            best_metrics = mae\n",
    "            best_predictions = predictions\n",
    "\n",
    "        print(f\"Iteration {iteration + 1} Results:\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"MAPE: {mape:.4f}\")\n",
    "        print(f\"Price Accuracy: {(1 - mape):.4f}\")\n",
    "        print(f\"Direction Accuracy: {dir_acc:.4f}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_mae = total_mae / n_iterations\n",
    "    avg_mape = total_mape / n_iterations\n",
    "    avg_acc = total_acc / n_iterations\n",
    "    avg_dir_acc = total_dir_acc / n_iterations\n",
    "\n",
    "    return avg_mae, avg_mape, avg_acc, avg_dir_acc, best_predictions\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    SEQUENCE_LENGTH = 60\n",
    "    N_ITERATIONS = 1\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X_train, y_train, X_test, y_test, scaler = load_and_preprocess_data(\n",
    "        \"XAU_15m_data.csv\",\n",
    "        split=0.85,\n",
    "        sequence_length=SEQUENCE_LENGTH\n",
    "    )\n",
    "    \n",
    "    print(\"Starting model training and evaluation...\")\n",
    "    mae, mape, acc, dir_acc, predictions = run_model(\n",
    "        X_train, y_train, X_test, y_test, scaler,\n",
    "        n_iterations=N_ITERATIONS,\n",
    "        sequence_length=SEQUENCE_LENGTH\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal Average Results:\")\n",
    "    print(f\"Mean Absolute Error = {mae:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error = {mape:.4f}\")\n",
    "    print(f\"Price Accuracy = {acc:.4f}\")\n",
    "    print(f\"Direction Accuracy = {dir_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_torch_imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
